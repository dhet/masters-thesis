\chapter{Realization} \label{chapter:realization}
After having discussed the basic concept of the approach in the previous chapter, in this chapter, an exemplary realization is presented. Previously, three key challenges were identified:
\begin{inparaenum}[(i)]
  \item \emph{reliable, anonymous information exchange},
  \item \emph{isolation}, and
  \item \emph{connectivity} (\cf \ref{sec:challenges}). 
\end{inparaenum}
To tackle these challenges, three technologies are proposed:

\begin{enumerate}[(i)]
\item \textbf{DDS} to enable \emph{reliable, anonymous information exchange},
\item \textbf{\docker} as containerization tool to achieve \emph{isolation} and portability of services, and
\item \textbf{\wnet} as means to provide \emph{connectivity} between the containerized services.
\end{enumerate}


\section{DDS as Messaging Middleware}

DDS is a promising candidate for a middleware that fulfills requirements to a satisfying degree.

OpenDDS was used.

Why DDS?

\paragraph{Versatile transport methods.} Supports shared memory (especially important), UDP and TCP transports

\paragraph{Dynamic Service Discovery.} Service discovery is extraordinarily fast and dynamic. Lost connections are picked up as soon as possible.

\paragraph{Data Centricity and Anonymous Messaging.}

\paragraph{Asynchronous Messaging.}

\paragraph{Location Transparency.}

\paragraph{Decentralization.}

\paragraph{Platform Independence.}


\subsection{Automatic Failover} \label{sec:failover}
DDS has ways to ensure reliable communication---even over unreliable transmission channels. Part of reliability is failure transparency, \ie , the ability to quickly substitute failed components by backup components. By means of certain DDS features, it is possible to realize such behavior. The substitution process involves two steps: first, the failure needs to be detected. Second, a backup service needs to be instructed to take over. Three QoS policies are needed to realize this process: \ownership , \ostrength\ and \liveliness\ (\cf \autoref{tab:qos}).

As the first step, a mechanism needs to determine whether a component has failed. In most cases, it is not possible for a failing component to properly shut down and "sign off", \ie , to notify the rest of the system that it will become unavailable. For this reason, the failure needs to be automatically registered. The \liveliness\ policy can be used to achieve this. \liveliness\ is the mechanism that determines whether a service is responsive ("alive"), or unresponsive ("dead"). The policy's value can be set to number indicating the maximum time interval between each liveliness signal. If a service fails to show a vital sign during that period, it is considered dead by the rest of the system. Passive components, \ie\ those which do not actively emit data, can be instructed to automatically send liveliness signals, or heartbeats, in certain intervals.

After a service has been declared dead it is up to the middleware to elect a substitute service. This is done through the \ownership\ and \ostrength\ QoS policies. By assigning a topic the \ownership\ value "\qos{exclusive}" one can specify that only a single data writer may write to that topic at any given time. Which data writer is given that prerogative is determined by the data writer's \ostrength\ value. The data writer that possesses the higher value is eligible to write to the topic. In the failover scenario, both, the primary service and the backup service are assigned to the same topic, and both are configured to have exclusive \ownership\ rights. The former one has a higher \ostrength\ than the latter one. Based on this value, the backup service can be instructed to take over.


\subsection{DDS for Automotive Systems}
Automotive software systems have previously relied -- and, to some degree, will continue to do so -- on low-level, low-bandwidth transport protocols such as CAN, LIN, etc. For the longest time, networks stacks based on those protocols were sufficient to meet the basic requirements of delivering vehicular sensor data and x-by-wire functions. However, driven by the emergence of innovative functions, the demands for vehicle intrinsic networks are skyrocketing. In particular, more and more sensor data from increasingly bandwidth-hungry sensors, such as cameras and LIDARs is feeding into advanced systems such as ADAS. At the same time, these functions require computational capabilities that go way beyond of what is possible with the microcontrollers typically used in traditional ECUs. High-performance computer systems based on high-level operating systems, supported by bandwidth-friendly networking protocols are needed to meet the new requirements. A new generation of low-level network protocols found their way into the vehicle. Notable mentions are FlexRay, and TSN. A question that remains is whether DDS is a suitable choice for the use within vehicles, and whether DDS may be used efficiently on top of these lower level protocols. \citeauthor*{bouhouch2013dds} have shown \cite{bouhouch2013dds} that DDS is indeed a suitable middleware to be used in vehicular networks.


DDS is designed for resource constrained real-time applications such as sensor networks or industrial automation.

DDS allows to configure how much of a system's resources an DDS-enabled application may use. Consequently, it is the middleware's responsibility to allocate resources as needed while still staying within the specified boundaries. At the same time, priorities aligning with the application's QoS settings need to be considered. DDS takes this burden off the programmer's shoulders.



\subsection{Separation of User Data}
\todo[inline]{nur eine idee}
The presented approach relies on a single cloud infrastructure, while at the same time, a vast number of customers need to be served. This poses a challenge concerning the separation of data. Confidentiality and privacy of user-specific application data must be preserved. Similarly, the result of a computation commissioned by a specific vehicle must be returned to exactly that vehicle, and no one else. 

Gladly, DDS offers a solution to this. In DDS, topics are not restricted to a single domain, \ie , they may be reused in multiple domains. If, \eg , a publisher belongs to \texttt{Domain $\alpha$} and publishes data on \texttt{Topic A}. Then, a data reader that reads from \texttt{Topic A} but belongs to \texttt{Domain $\beta$} may not read the data. Therefore, through domains, the same application may be reused several times, while keeping topic data separate. This principle is take advantage of in the presented approach: Domains are used to separate user data, such that for each user there is one user-specific domain.

%
%
%
%
%
%
%
%
%
%

\section{\docker\ as Containerization Technology}
\paragraph{Portability} Containers need to run in vehicles and cloud -> \docker is available on many platforms, ARM version is fully functional

Besides providing isolation, \docker\ has a number of other benefits. 
\paragraph{Innovation Pressure}(not docker specific) calls for fast development cycles -> CI/CD.

\paragraph{Light-weight}: Important for embedded systems, provides: fast spin-up, low overhead

\paragraph{Flexible network interfaces}: provide connection between distributed containers

\paragraph{layered images}: save disk space

\paragraph{Isolation}: how is isolation achieved?


\subsection{Multi Platform Compatibility} 
The primary purpose of containerization is to provide portable execution environments that allow software to run on a broad range of computing systems. A limitation of containers is that they only provide portability across \emph{operating systems}, and not \emph{hardware platforms}. In other words, containers do not provide binary compatibility. The reason for this is that containerized applications run directly on the kernel of the host system, and do not build on top of a virtualization layer as classic VMs do. As a result, a containerized binary built for an x86-based processor will not run on an ARM system, and vice versa. This is problematic especially in the context of embedded systems which often rely on particular hardware architectures that favor energy efficiency over performance. Computing nodes in a data centers, on the other hand, are typically based on architectures aimed at providing a maximum level of performance. As the presented approach intends containers to be deployed on both, vehicular on-board systems and in the cloud, this poses a challenge. 

Several approaches exist to tackle this problem. For example, one could embed a thin platform compatibility layer at the base of each container. This approach was popularized by \emph{resin.io}\footnote{\url{www.resin.io}}. resin.io is a company that specializes in containerization for IoT devices. On their Docker Hub page\footnote{\url{hub.docker.com/u/resin}} they provide \docker\ images which have the \emph{QEMU}\footnote{\url{www.qemu.org}} machine emulator built in. Facilitated by QEMU's user emulation mode, binaries built for a given processor architecture may be executed on otherwise incompatible processors. QEMU achieves this by translating any guest system calls into host system calls. The previously mentioned images are built in a way that allows any arbitrary binary executed within such container to be run in the context of QEMU. Thus, the container may run on a multitude of hardware platforms.
This approach simplifies the software build process tremendously as only one container per service needs to be built. This container can then be reused for all platforms. However, this way of achieving multi-platform portability turned out to be unsuitable for the intended use case as multicast is not well supported by QEMU. 

Thus, another approach was leveraged to solve the multi-platform compatibility issue. This approach builds on Continuous Integration (CI) in accord with Docker Hub, and in particular, Docker Hub's \emph{image manifest} feature. By default, a \docker\ image name corresponds to exactly one image. Thus, whenever an image with a given name is requested from the \docker\ repository, only that specific image is returned to the requester. Image manifests extend repositories by the option to define \emph{multiple} images per image name. That way, several containers, each specifically built for a given platform, may be deployed under the same name. Depending on the requesting node's hardware architecture, a different image may be returned. \autoref{fig:manifest-pull} depicts an example for this. In the example, an x86-based machine requests the \texttt{some/image:v1.0} image from the repository. What is then returned from the repository is an image specifically made for x86 architectures. On the other hand, an ARM-based machine which requests the very same image, \texttt{some/image:v1.0}, receives an entirely different image as a response. This kind of behavior is achieved through an image look-up in the manifest file which is performed behind the scenes. 

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/manifest-pull}
  \caption[Pulling \docker\ images via manifest file]{Two nodes request the same image name but receive different images which are specifically built for their respective processor architecture. Which node requires which image is determined by a look-up in the manifest file.}\label{fig:manifest-pull} 
\end{figure}

A disadvantage of this approach (compared to the QEMU one) is that multiple versions of the same container need to be built every time a service receives a software update. To cope with this hindrance, a minimal CI pipeline was leveraged. Consider \autoref{fig:manifest-push} in which the employed service deployment process is depicted. The build process is initiated whenever a code change is pushed to the remote git repository. Once a push is registered, the CI Platform (Travis CI\footnote{\url{www.travis-ci.org}} was used) pulls the latest version from the git repository and executes a build script. The build script compiles several version of the service for each target platform, embeds the binaries in images and pushes those images to the image repository (Docker Hub).

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{figures/manifest-push}
  \caption[Pushing \docker\ images via CI]{A git push to a service's git repository triggers the build process in the CI platform: the code is compiled for several platforms and the resulting images are pushed to the image repostory.}\label{fig:manifest-push} 
\end{figure}

%
%
%
%
%
%
%
%
%
%
%
\subsection{Containerized Services}
In the envisaged system, each service is packed into its own Linux container with all its dependencies. That way, a great deal of portability is achieved, \ie , services may run wherever they are placed. The approach greatly benefits from \docker 's layered images.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{figures/docker-sharing}
  \caption[An example of containerized services]{An example of four containerized service instances, some sharing common logic to save disk space and facilitate fast updates}\label{fig:service-containers} 
\end{figure}

\autoref{fig:service-containers} shows an example of three containerized services running on two different machines, \experim{Host I} and \experim{Host II}. The former hosts two instances of \experim{Service A}, and one instance of \experim{Service B}. On the second host only one service instance is running, namely one of type \experim{Service C}. All services are packaged in their own, separate container. The containers are made up of three stacked images---the bottom two are common to all services. The bottommost image ("\emph{base image}") contains the file system layers of a minimal operating system. In this thesis, the Linux distribution \emph{Debian}\footnote{\url{www.debian.org}} was used. The base image only contains a limited selection of Linux tools, such as \texttt{ls}, \texttt{cat}, etc. The purpose of a base image is to lay a solid foundation that enables users to work comfortably within the container. In the case of Debian, the \emph{aptitude} package manager is additionally included which enables users to easily install further tools. 

Next, an \emph{intermediate image} is built on top of the base image. The intermediate image adds further layers containing the \emph{run-time environment}, and in particular, the shared libraries of OpenDDS. This image could optionally contain more libraries and tools that are common to all services. For the purpose of demonstration, however, DDS is sufficient\todo{updaten, falls es zu case study kommt} . The base image and the intermediate image lay the foundation of all services. Any image built on top of these could run an DDS-enabled service. \docker 's layered images allows these layers to be shared \todo{besser erklaeren}

At the top, finally, sit the \emph{service images}. In these images contained is the actual service logic, and more precisely, the service's binary and configuration files. The service images are unique to each service, and are not shared, except when the same service is instantiated multiple times on the same machine (\cf \experim{Service A} in \autoref{fig:service-containers}).


\input{chapters/sections/weave}








