\chapter{Realization} \label{chapter:realization}
After having discussed the basic concept of the approach in the previous chapter, in this chapter, an exemplary realization is presented. Previously, three key challenges were identified:
\begin{inparaenum}[(i)]
  \item \emph{reliable, anonymous information exchange},
  \item \emph{isolation}, and
  \item \emph{connectivity} (\cf \ref{sec:challenges}).
\end{inparaenum}
To tackle these challenges, three technologies are proposed:

\begin{enumerate}[(i)]
\item \textbf{DDS} to enable \emph{reliable, anonymous information exchange},
\item \textbf{\docker} as containerization tool to achieve \emph{isolation} and portability of services, and
\item \textbf{\wnet} as means to provide \emph{connectivity} between the containerized services.
\end{enumerate}

In the following, these three tools are presented and it is described how they are leveraged to realize the proof of concept implementation.


\input{chapters/sections/dds}

%
%
%
%
%
%
%
%
%
%
\pagebreak
\section{\docker\ for Containerization}
In this thesis' approach, services and their dependencies shall be encapsulated in their own, self-contained execution environments that may be deployed inside the vehicle and also in the cloud. Containerization is a promising technology to achieve this. For the exemplary implementation, \docker\ is used as it provides an easy-to-use interface and is available for x86, as well as for ARM-based platforms.

\docker\ is a holistic containerization platform which includes everything needed to build, run, deploy, and distribute containers. The \docker\ application is structured in a client-server model (\cf \Cref{fig:docker-arch}). The server is a daemon process running at all times on the host machine. Its primary purpose is to manage containers, images, networks and data volumes, and to provide the means to  build images and run containers. The \docker\ server exposes a RESTful HTTP API by which it can be controlled. The client side of the \docker\ application is a command line interface (CLI) which acts as a user friendly fa√ßade for the server's API.\footnote{\url{docs.docker.com/engine/docker-overview}}

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/docker-arch.pdf}
  \caption[\docker\ architecture]{\docker\ architecture}\label{fig:docker-arch}
\end{figure}

\pagebreak
\subsection{\docker\ Components}
\paragraph{\docker\ Images.}
Images are a central component of \docker . They may be seen as the ``blueprint'' on the basis of which containers are built and define exactly which files and directories are contained in a container once it comes to life. Effectively, every container is an instantiation of an image of a certain type, in the same way an object is an instantiation of a class in an object-oriented programming language. While the concept of images is very common among containerization and virtualization tools, \docker\ follows a rather unconventional approach.
In \docker , images are made up of series of read-only file system layers stacked on top of each other. Each layer may add files and directories to its respective underlying layer. If a layer tries to modify a file from an underlying layer it creates a copy of that file and adds it to its own layer. The modification is then performed on the copy instead of the original. This principle called ``Copy on Write'' (CoW) ensures that layers cannot be modified which makes it possible to share individual layers with other images stored on the host. An example of this is later described in \Cref{sec:containerized-services}. By treating images as a collection of individual, sharable pieces instead of atomic units, massive storage space savings can be achieved. Another benefit of this is that whenever a container is started, only a thin writable layer needs to be added on top of an image, and the underlying layers do not need to be copied. That way, start-up times of containers are kept extremely low.\footnote{\url{docs.docker.com/storage/storagedriver}} Virtual Machines, in contrast, would first need to boot into an operating system prior to operation.

\paragraph{Dockerfiles.}
\docker\ images are defined in so-called Dockerfiles.\footnote{\url{docs.docker.com/engine/reference/builder}} Dockerfiles are made up of series of instructions that tell \docker\ how to build a certain image. Each instruction adds another file system layer to the image.
\begin{lstlisting}[caption=An examplary Dockerfile, label=lst:dockerfile, numbers=left, numberstyle=\tiny]
FROM debian
COPY . /application
RUN cd /application && make
ENTRYPOINT /application/run.sh
\end{lstlisting}
An example of a Dockerfile is depicted in \Cref{lst:dockerfile}. The first line of this Dockerfile defines the base image on top of which this particular image shall be built. In this case, an image called ``debian'' was chosen. As the name suggests, that base image contains a basic installation of the Linux distribution Debian.\footnote{\url{www.debian.org}} Next, the line \ \mbox{\texttt{COPY . /application}} \ instructs \docker\ to copy the contents of the current (host) directory to the container's \ \mbox{\texttt{/application}} \ directory. Due to \docker 's CoW approach, the changes in the file system are added in the form of a layer---the underlying layers are not affected. The \ \texttt{RUN} \ instruction in the following line causes \docker\ to run the subsequent commands within the container. In this case, the newly added application directory is entered and the command \ \texttt{make} \ is executed, effectively compiling the application. Again, the resulting changes to the file system are encapsulated within another layer. In practice, the now topmost layer would contain all files produced by \ \texttt{make}. Finally, the \ \mbox{\texttt{ENTRYPOINT}} \ instruction defines the action to be performed once the container is started. In this case, the shell script \ \texttt{run.sh} \ would be executed within the container. This command, too, would add a layer to the image, albeit an empty one. By defining images as sequences of instructions, reproducibility is guaranteed.
By running the command \ \mbox{\texttt{docker build -t IMAGENAME .}} \ in the directory of the Dockerfile, an image based on that Dockerfile would be built. Using the command \ \mbox{\texttt{docker run IMAGENAME}}, a new container of that image type would be launched.

\paragraph{\docker\ Registries.}
\docker\ images may be stored in \docker\ Registries,\footnote{\url{docs.docker.com/registry}} where they can be browsed, managed, and distributed. The command \ \mbox{\texttt{docker push IMAGENAME[:TAG]}} \ causes the image with the name \ \mbox{\texttt{IMAGENAME}} \ to be uploaded to a registry. Analogously, to download an image from a registry to the local computer, one would have to run the command \ \mbox{\texttt{docker pull IMAGENAME[:TAG]}}. \docker\ registries are implemented as a server software which exposes an HTTP API (\cf \Cref{fig:docker-arch}). The registry server is open source\footnote{\url{www.github.com/docker/distribution}} and may be deployed on any server that supports it. The \docker\ company itself hosts one of such registries, Docker Hub,\footnote{\url{hub.docker.com}} which is provided as a publicly available  and free-to-use service.

\subsection{Multi-Platform Compatibility} \label{sec:multiplat}
The primary purpose of containerization is to provide portable execution environments that allow software to run on a broad range of computing systems. A limitation of containers is that they only provide portability across \emph{operating systems}, and not \emph{hardware platforms}. In other words, containers do not provide binary compatibility. The reason for this is that containerized applications run directly on the kernel of the host system, and do not build on top of a virtualization layer as classic VMs do. As a result, a containerized binary built for an x86-based processor will not run on an ARM system, and vice versa. This is problematic especially in the context of embedded systems which often rely on particular hardware architectures that favor energy efficiency over performance. Computing nodes in a data centers, on the other hand, are typically based on architectures aimed at providing a maximum level of performance. As the presented approach intends containers to be deployed on both, vehicular on-board systems and in the cloud, this poses a challenge. Two approaches exist to tackle this problem.

\subsubsection{Universal, QEMU-enabled Images}
The first approach relies on a thin platform compatibility layer that is put into the base of each container. This approach was popularized by resin.io, a company that specializes in containerization for IoT devices. On their Docker Hub page\footnote{\url{hub.docker.com/u/resin}} they provide \docker\ images which have the QEMU\footnote{\url{www.qemu.org}} machine emulator built in. Facilitated by QEMU's user emulation mode, binaries built for a given processor architecture may be executed on otherwise incompatible processors. QEMU achieves this by translating any guest system calls into host system calls. The previously mentioned images are built in a way that allows any arbitrary binary executed within such container to be run in the context of QEMU. Thus, the container may run on a multitude of hardware platforms.
This approach simplifies the software build process tremendously as only one universal container per service needs to be built. This container can then be reused for all platforms.

\subsubsection{Manifest Lookup}
The QEMU approach has a significant drawback: multicast is not well supported by QEMU, which is why the idea was discarded.
Thus, another approach was leveraged to solve the multi-platform compatibility issue. This approach builds on Continuous Integration (CI) in accord with Docker Hub, and in particular, Docker Hub's \emph{image manifest} feature. The concept of image manifests builds on the following premise: by default, a \docker\ image name corresponds to exactly \emph{one} image. Thus, whenever an image with a given name is requested from the \docker\ registry, only that specific image is returned to the requester. Image manifests extend registries by the option to define \emph{multiple} images per image name. That way, several containers, each specifically built for a given platform, may be deployed under the same name. Depending on the requesting node's hardware architecture, a different image may be returned. \Cref{fig:manifest-pull} depicts an example for this. In the example, an x86-based machine requests the \ \mbox{\texttt{some/image:v1.0}} \ image from the registry. What is then returned from the registry is an image specifically made for x86 architectures. On the other hand, an ARM-based machine which requests the very same image, \ \mbox{\texttt{some/image:v1.0}}, receives an entirely different image as a response. This kind of behavior is achieved through an image lookup in the manifest file which is performed behind the scenes.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/manifest-pull}
  \caption[Pulling \docker\ images via manifest file]{Two nodes request the same image name but receive different images which are specifically built for their respective processor architecture. Which node requires which image is determined by a look-up in the manifest file.}\label{fig:manifest-pull}
\end{figure}

A disadvantage of this approach (compared to the QEMU one) is that multiple versions of the same container need to be built every time a service receives a software update. To cope with this hindrance, a minimal CI pipeline was leveraged. Consider \Cref{fig:manifest-push} in which the employed service deployment process is depicted. The build process is initiated whenever a code change is pushed to the remote git repository. Once a push is registered, the CI Platform (Travis CI\footnote{\url{www.travis-ci.org}} was used) pulls the latest version from the git repository and executes a build script. The build script compiles several version of the service for each target platform, embeds the binaries in images and pushes those images to the \docker\ registry (Docker Hub). Thus, several images, each tailored to a different platform, may be built and deployed in one go.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{figures/manifest-push}
  \caption[Pushing \docker\ images via CI]{A git push to a service's git repository triggers the build process in the CI platform: the code is compiled for several platforms and the resulting images are pushed to the \docker\ registry.}\label{fig:manifest-push}
\end{figure}

%
%
%
%
%
%
%
%
%
%
%
\subsection{Containerized Services} \label{sec:containerized-services}
In the envisaged system, each service is packed into its own container with all its dependencies. That way, services may run wherever they are placed, and thus, a great deal of portability is achieved.

\Cref{fig:service-containers} shows an example of three containerized services running on two different machines, \experim{Host I} and \experim{Host II}. On the former, two instances of \experim{Service A}, and one instance of \experim{Service B} are placed. On the second host only one service instance is deployed, namely one of type \experim{Service C}. All services are packaged in their own, separate container. The containers are made up of three stacked images---the bottom two are common to all services. The bottommost image (``\emph{base image}'') contains the file system layers of a minimal operating system. In this thesis, the Linux distribution Debian was used. The base image only contains a limited selection of Linux tools, such as \ \texttt{ls}, \texttt{cat}, \texttt{ps} \ etc. The purpose of a base image is to lay a solid foundation that enables users to work comfortably within the container. In the case of Debian, the \emph{aptitude} package manager is additionally included which enables users to easily install further tools.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{figures/docker-sharing}
  \caption[An example of containerized services]{An example of four containerized service instances, some sharing common logic to save disk space and facilitate fast updates}\label{fig:service-containers}
\end{figure}

Next, an \emph{intermediate image} is built on top of the base image. The intermediate image adds further layers containing the services' run-time environment, and in particular, the shared libraries of OpenDDS. This image could optionally contain additional libraries and tools that are common to all services. For the purpose of demonstration, however, DDS on its own is sufficient. The base image and the intermediate image lay the foundation of all services. Any image built on top of these could run any DDS-enabled application. Facilitated by \docker 's layered approach to images, the bottom two images can be shared among all services deployed on a host. In the example, the base image and the intermediate image both only need to be stored once on \experim{Host I}, which saves tremendous amounts of disk space.

At the top, finally, sit the \emph{service images}. In these images contained is the actual service logic, and more precisely, the service's binary and configuration files. The service images are unique to each service, and are not shared, except when the same service is instantiated multiple times on the same machine (\cf \experim{Service A} in \Cref{fig:service-containers}).


\input{chapters/sections/weave}








